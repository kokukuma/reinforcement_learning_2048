2d_random_walk
===========================

## 検証問題.
+ 2次元ランダムウォーク
  + 6x6
  + (0,0)に到達したら報酬.
  + (5,5)に到達したら報酬なし終了.
  + 最大100回cycleまで.
  + 報酬は移動距離に関わらず一律.
  + 初期状態 : (2, 2) or random


## baseモデル
+ モデル
  + NN : 多層パーセプトロン(14, 14, 1)
    + 入力: agentの位置(12) + agentの行動(4)
    + 出力: Q値
    + 中間層:logistic sigmoid, 出力層:線形
    + 学習方法: iRPROP-
  + 各種パラメータ
    + 入力値: ダミー変数化
    + 割引率: decay=0.9
    + トレーニング時:  epsilon=1.0にする.
    + 総学習episode : 100
    + 1回の学習に置けるepisode : 10

+ 評価方法
  + 初期値の影響を考慮し, トレーニングから評価までを50回繰りし平均を取る.
  + 確認する指標
    + ゴール到達割合
    + ゴールまでの平均step数
    + 学習前後のQ値最大の行動方向

+ 結果
  + 平均得点: 1000.0(0)
  + 平均step: 6.62(4.6)
  + Q値最大の方向
    + 学習前Q値最大の行動方向
> 5  :  → → → → → →
> 4  :  → → → → → →
> 3  :  → → → → → →
> 2  :  → → → → → →
> 1  :  → → → → → →
> 0  :  → → → → → →

    + 学習後Q値最大の行動方向
> 5  :  ↓ ↓ ← ← ← ←
> 4  :  ↓ ↓ ↓ ← ↓ ←
> 3  :  ↓ ↓ ↓ ← ↓ ←
> 2  :  ↓ ← ↓ ↓ ↓ ↓
> 1  :  ↓ ← ↓ ← ← ↓
> 0  :  ↓ ← ← ← ← ↓

  + 50回行い, 全て(0,0)まで到達した.
  + 学習前後でQ値最大の方向は, 報酬が受け取れる(0,0) に集中していることが分かる.

## 特性調査
### 総学習episodes
+ 内容
  + 1回の学習に置けるepisodeは変更せず, 総学習episodeを変化させる.
  + つまり, 学習する回数を増加させる.
  + total_episode = 10, 100(default), 500
+ 結果
  + ある程度まで行ったら, 学習回数増やしても効果なし.

  | 回数  | 平均得点     |  平均到達step  |
  |-------|--------------|----------------|
  | 10    | 440.0(496)   |  64.86(40)     |
  | 100   | 1000.0(0)    |  6.62(4.6)     |
  | 1000  | 1000.0(0)    |  6.7(6.3)      |

### 1学習episodes
+ 内容
  + 学習の回数を固定して, 1回の学習に置けるepisodeは変更する.
  + つまり, 学習の質を向上させる.
  + (total_episode, episode) = (10,1), (100, 10)(default), (500, 50), (1000, 100)
+ 結果
  + 50以上上げても特に変化なし.
  + 問題の取りうる状態がもっと多くなったら, もっと上がる必要があるかもしれない.
  + 1episodeでは, 学習を10回以上に増やしても, 上手く学習はできなかった.
  + これの原因は下でも上げられているように, 訓練データ同士が競合して上手く学習できないとか?
    + http://shws.cc.oita-u.ac.jp/shibata/pub/ADS01.pdf
  + それに対する対策としては
    + 競合しないように訓練データを局所化するか
    + 競合しても上手く学習できるようにmini-batchで学習するか.

    | 回数    | 平均得点       | 平均到達step     |
    | ------- | -------------- | ---------------- |
    | 1       | 360.0(480)     | 61.72(38.3)      |
    | 10      | 1000.0(0.0)    | 6.62(4.6)        |
    | 50      | 1000.0(0.0)    | 5.32(2.1)        |
    | 100     | 1000.0(0.0)    | 5.84(2.9)        |


### 学習パラメータ
+ 内容
  + 学習時の割引率, 探索時のepsilonを変化させた場合, 学習結果に与える影響を調べる.
  + decay   = 0.1, 0.5, 0.9(default)
  + epsilon = 0.3, 0.6, 1.0(default)
+ 結果
  + 割引率は, 極端に低くなければそれほど影響なし.
  + ただこれは, 報酬が得られる状態に通じる状態[(0,1), (1,0)] の出現回数が多い問題だからだとも考えられる.
  + 状態の数が多く, 正解に通じる状態の出現回数が少なくなる場合, もっと影響は大きくなると思う.
  + まぁ何にしても, 0.9より下げる必要はない.

  | decay | 平均得点    | 平均到達step  |
  |-------|-------------|---------------|
  | 0.1   | 980.0(140)  | 7.96(13.6)    |
  | 0.5   | 1000.0(0.0) | 5.8(3.0)      |
  | 0.9   | 1000.0(0.0) | 6.62(4.6)     |

  + 下げるほど, 得点も到達stepも減少する.
  + これは, 探索にランダム性がなくなるにつれて, 学習に利用されるepisodeの種類が減少するためだと考えられる.
  + つまり, 学習の質が下がるからだと考えられる.

  |epsilon| 平均得点    | 平均到達step  |
  |-------|-------------|---------------|
  | 0.3   | 700.0(458)  | 33.68(40.0)   |
  | 0.6   | 960.0(195)  | 13.42(21.6)   |
  | 0.9   | 1000.0(0.0) | 6.62(4.6)     |


### 報酬レンジ
+ 内容
  + 与える報酬の影響を調べる.
  + reward = 1, 10, 100, 1000(default)
+ 結果
  + 10以降, 報酬を大きくしても, それほど改善は見られない.
  + 初期のQ値を確認すると, 0.7とかそこそこだった.
  + 報酬1とかだと, 初期値の影響が大きすぎて上手く学習できないぽい.
  + 初期のQ値を超えるような報酬設定ならOK.

  | 報酬  | 平均得点     |  平均到達step  |
  |-------|--------------|----------------|
  | 1     | 0.5(0.5)     | 60.72(40)      |
  | 10    | 10.0(0.0)    | 5.78(2.5)      |
  | 100   | 100.0(0.0)   | 5.96(2.7)      |
  | 1000  | 1000.0(0.0)  | 6.62(4.6)      |

### 初期位置
+ 内容
  + 学習の際, 初期位置を固定ではなく, ランダムにすることによる影響を調べる.
  + これにより, 学習に使われるepisodeの種類が増え, 質の高い学習ができることが期待される.
+ 結果
  + 確かに, randomの方が, 平均step回数もその標準偏差も小さくなった.

  | 報酬     | 平均得点     |  平均到達step  |
  |----------|--------------|----------------|
  | 固定     | 1000.0(0)    |  6.62(4.6)     |
  | ランダム | 1000.0(0)    |  5.74(2.3)     |

## まとめ
+ 学習回数, 学習の質をある程度上げないと, 上手く学習できない.
+ 報酬は, 初期のQ値を超えるほどの大きさでなければ, 上手く学習できない.

