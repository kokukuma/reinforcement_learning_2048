2d_random_walk
===========================

## 検証問題.
+ 2次元ランダムウォーク
  + 6x6
  + (0,0)に到達したら報酬.
  + (5,5)に到達したら報酬なし終了.
  + 最大100回cycleまで.
  + 報酬は移動距離に関わらず一律.
  + 初期状態 : (2, 2) or random

## baseモデル
+ NN : 多層パーセプトロン(14, 14, 1)
  + 入力: agentの位置(12) + agentの行動(4)
  + 出力: Q値
  + 中間層:logistic sigmoid, 出力層:線形
+ 学習方法: RPROP
+ 入力値: ダミー変数化
+ 割引率: decay=0.9
+ トレーニング時:  epsilon=1.0にする.
+ 総学習episode : 100
+ 1回の学習に置けるepisode : 10

## 評価指標
+ トレーニングから評価までを50回繰り返す.
+ 確認する指標 
  + ゴール到達割合
  + ゴールまでの平均step数
  + Q値の状態

## 特性調査
### 総学習episodes
+ 内容
  + 1回の学習に置けるepisodeは変更せず, 総学習episodeを変化させる.
  + つまり, 学習する回数を増加させる.
  + total_episode = 10, 100(default), 500
+ 結果
  + ある程度まで行ったら, 学習回数増やしても効果なし.
  | 回数  | 平均得点     |  平均到達step  |
  +-------+--------------+----------------+
  | 10    | 440.0(496)   |  64.86(40)     |
  | 100   | 1000.0(0)    |  6.62(4.6)     |
  | 1000  | 1000.0(0)    |  6.7(6.3)      |

### 1学習episodes
+ 内容
  + 学習の回数を固定して, 1回の学習に置けるepisodeは変更する.
  + つまり, 学習の質を向上させる.
  + (total_episode, episode) = (10,1), (100, 10), (500, 50), (1000, 100)
+ 結果
  + 
  | 回数  | 平均得点     |  平均到達step  |
  +-------+--------------+----------------+


### 学習パラメータ
+ 内容
  + 学習時の割引率, 探索時のepsilonを変化させた場合, 学習結果に与える影響を調べる.
  + decay   = 0.1, 0.5, 0.9(default)
  + epsilon = 0.3, 0.6, 1.0(default)
+ 結果
  + 割引率は, 極端に低くなければそれほど影響なし.
  + ただこれは, 報酬が得られる状態に通じる状態[(0,1), (1,0)] の出現回数が多い問題だからだとも考えられる.
  + 状態の数が多く, 正解に通じる状態の出現回数が少なくなる場合, もっと影響は大きくなると思う.
  + まぁ何にしても, 0.9より下げる必要はない.
  | decay | 平均得点    | 平均到達step  |
  +-------+-------------+---------------+
  | 0.1   | 980.0(140)  | 7.96(13.6)    |
  | 0.5   | 1000.0(0.0) | 5.8(3.0)      |
  | 0.9   | 1000.0(0.0) | 6.62(4.6)     |

  + 下げるほど, 得点も到達stepも減少する.
  + これは, 探索にランダム性がなくなるにつれて, 学習に利用されるepisodeの種類が減少するためだと考えられる.
  + つまり, 学習の質が下がるからだと考えられる.
  |epsilon| 平均得点    | 平均到達step  |
  +-------+-------------+---------------+
  | 0.3   | 700.0(458)  | 33.68(40.0)   |
  | 0.6   | 960.0(195)  | 13.42(21.6)   |
  | 0.9   | 1000.0(0.0) | 6.62(4.6)     |


### 報酬レンジ
+ 内容
  + 与える報酬の影響を調べる.
  + reward = 1, 10, 100, 1000(default)
+ 結果
  + 10以降, 報酬を大きくしても, それほど改善は見られない.
  + 初期のQ値を超えるような報酬設定ならOK.
  | 報酬  | 平均得点     |  平均到達step  |
  +-------+--------------+----------------+
  | 1     | 0.5(0.5)     | 60.72(40)      |
  | 10    | 10.0(0.0)    | 5.78(2.5)      |
  | 100   | 100.0(0.0)   | 5.96(2.7)      |
  | 1000  | 1000.0(0.0)  | 6.62(4.6)      |

### 初期位置
+ 内容
  + 学習の際, 初期位置を固定ではなく, ランダムにすることによる影響を調べる.
  + これにより, 学習に使われるepisodeの種類が増え, 質の高い学習ができることが期待される.
+ 結果
  + 


## まとめ
+ 学習回数, 学習の質がある程度上げないと, 上手く学習できない.
+ 報酬は, 初期のQ値を超えるほどの大きさでなければ, 上手く学習できない.

