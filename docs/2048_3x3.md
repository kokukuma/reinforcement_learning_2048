2048_3x3
===========================

## 2048 - 3x3に適用してみる.
+ とりあえず, やってみる.
  + 入力はダミー変数化
  + 報酬は最後に取ったやつ全部与える.
  + 状態数が多いので, 1回の学習で,
    + 1000 episode を100 episodeずつ学習させる.
  + 同じ状態がなかなかでないと思うので, learn_count=5とした.
  + randomで最後まで到達できないと考えられるので,
    + 訓練中もgreedly=0.7
    + 訓練は100回繰り返す.
  + つまり, 1回の学習で, 1000episodeを５回学習する.

+ 結果
  + 学習が全く進まない.
  + 失敗...

## 原因検討
+ 状態の種類多すぎて, Q値が更新されない.
  + 3x3-13種でも, 取りうる状態は, 13^9
  + ランダム状態では, 1gameで30くらいの状態が観察できる.
  + 1トレーニングで100 gameとすると13^9/3000 = 3,534,833回.
  + 300万回トレーニングしないと, 全ての状態を評価できない.
  + まぁ, 上の方は出ないだろうから, 半分以下ではあるはず.

+ 学習を続けたとき観測される状態数を確認.
  + (この状態数確認したとき, どんな条件で計算したか記録するの忘れた....)
  + 5万5千間で来て, まだ頭打ちの気配なし.
  + 1回 : 27,000
  + 5回の学習で, 13万超えた.
    => 学習300回くらいで多少変わるかも.

+ 報酬の与え方を変えてみた.
  + ま, 変わりないな.


##  状態過多への対策
+ 大量に学習する.
  + 大量のエピソードを学習させる.
  + 辛い... 

+ 1学習の効率を上げる.
  + 同じエピソードを何度も学習する.
    + その回数分の報酬までのステップまで, Q値が反映される.
    + lib/pybrain/rl/agents/learning.pyのepisodesを上げるような.
  + 未確認状態のQ値を予想する.
    + ファジィQ-learning
    + [ファジイ推論](http://www.sist.ac.jp/~kanakubo/research/reasoning_kr/fuzzy.html)
    + [Fuzzy Inference Model for Learning from Experiences and Its Application to](http://www.aso.ecei.tohoku.ac.jp/publication_data/377.pdfRobot Navigation)

+ 状態を減らす.
  + 状態の特徴量を求め, 学習を行う(次元圧縮して学習する).
    + 何を基準に同じ特徴を持った状態と判断するか?
    + 学習した結果によって, どの状態を同じと見なすか更新したい. 
  + 複数のagentを組み合わせて, 1つのagentが担当する状態を減らす.
    + [複数の学習器の階層的構築による行動獲得](http://www.er.ams.eng.osaka-u.ac.jp/Paper/2000/Takahashi00d.pdf) 
    + [階層型学習機構における状態行動空間の構成](http://www.er.ams.eng.osaka-u.ac.jp/Paper/2003/Takahashi03a.pdf) 
    + 例えば, 0と2, 0と4だけ認識するagentを組み合わせて学習させるのはどうだろうか. 




